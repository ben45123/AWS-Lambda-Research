{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben45123/AWS-Lambda-Research/blob/main/rag_news_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install the necessary packages\n",
        "!pip install pinecone langchain langchain_openai openai langgraph pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRW2E-XKD2sM",
        "outputId": "4c58a5b4-cebb-4132-b78f-fedb3f130331"
      },
      "id": "fRW2E-XKD2sM",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.0)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.3)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.1.31)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.13.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.34)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.25)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.64)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (24.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import libraries and set API keys\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "import pinecone\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set API keys from Colab userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')  # Or use your provided key if saved\n",
        "# If you're using the key from your provided code\n",
        "# PINECONE_API_KEY = \"pcsk_6akU8Z_2BXXXDSBKbvFCn4sciNM2FeJC6PwAt6wFwQeQjoJKDSjysRbtyBAdUfRv6z87e6\"\n",
        "\n",
        "# Set environment variables (some LangChain components use these)\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# Configuration for Pinecone\n",
        "PINECONE_INDEX_NAME = \"cus635\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "NAMESPACE = \"Team_1\"\n",
        "CATEGORY = \"Finance\"\n",
        "\n",
        "print(\"API keys loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLmzYwKpD3Jj",
        "outputId": "6fe77c32-a581-4930-fec4-a5afd3a4d9dc"
      },
      "id": "oLmzYwKpD3Jj",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Import LangChain and LangGraph components\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Initialize models\n",
        "embeddings = OpenAIEmbeddings()\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "print(\"LangChain components initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpG4H3PkD7I4",
        "outputId": "94b1d573-c9bd-4b2b-d287-3054cbbdde0d"
      },
      "id": "SpG4H3PkD7I4",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain components initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Create the NewsRAG class for basic RAG functionality\n",
        "class NewsRAG:\n",
        "    \"\"\"\n",
        "    A RAG system that uses LangChain to answer questions based on news articles\n",
        "    stored in Pinecone.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, index_name: str, namespace: str, category: str):\n",
        "        \"\"\"\n",
        "        Initialize the NewsRAG with Pinecone credentials and team information.\n",
        "\n",
        "        Args:\n",
        "            api_key: Pinecone API key\n",
        "            index_name: Pinecone index name\n",
        "            namespace: Namespace (team name)\n",
        "            category: News category\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.index_name = index_name\n",
        "        self.namespace = namespace\n",
        "        self.category = category\n",
        "        self.pinecone_index = None\n",
        "        self.retriever = None\n",
        "        self.rag_chain = None\n",
        "        self.conversational_memory = []\n",
        "\n",
        "    def connect_to_pinecone(self):\n",
        "        \"\"\"Connect to Pinecone and initialize the index.\"\"\"\n",
        "        try:\n",
        "            # Initialize Pinecone\n",
        "            pinecone_client = pinecone.Pinecone(api_key=self.api_key, environment=\"us-east-1\")\n",
        "            self.pinecone_index = pinecone_client.Index(self.index_name)\n",
        "            print(f\"Connected to Pinecone index: {self.index_name}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error connecting to Pinecone: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def initialize_retriever(self):\n",
        "        \"\"\"Initialize the retriever from the Pinecone index.\"\"\"\n",
        "        try:\n",
        "            # Create a LangChain vectorstore\n",
        "            vectorstore = LangchainPinecone(\n",
        "                index=self.pinecone_index,\n",
        "                embedding=embeddings,\n",
        "                text_key=\"text\",\n",
        "                namespace=self.namespace\n",
        "            )\n",
        "\n",
        "            # Create the retriever with filters\n",
        "            self.retriever = vectorstore.as_retriever(\n",
        "                search_kwargs={\n",
        "                    \"k\": 5,\n",
        "                    \"filter\": {\"category\": self.category}\n",
        "                }\n",
        "            )\n",
        "\n",
        "            print(f\"Retriever initialized for category: {self.category}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing retriever: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def initialize_rag_chain(self):\n",
        "        \"\"\"Initialize the basic RAG chain.\"\"\"\n",
        "        # Define the prompt template\n",
        "        template = \"\"\"You are an AI assistant specialized in analyzing news articles in the {category} domain.\n",
        "        Use the following retrieved news articles to answer the question.\n",
        "\n",
        "        Retrieved articles:\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer the question based on the retrieved articles. If the retrieved articles don't contain the information\n",
        "        needed to answer the question accurately, acknowledge the limitations and provide the best answer possible\n",
        "        based on available information. Include relevant sources in your response.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create the prompt\n",
        "        prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "        # Create the RAG chain\n",
        "        self.rag_chain = (\n",
        "            {\"context\": self.retriever, \"question\": RunnablePassthrough(), \"category\": lambda _: self.category}\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        print(\"Basic RAG chain initialized\")\n",
        "        return True\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Query the RAG system with a question.\n",
        "\n",
        "        Args:\n",
        "            question: User's question\n",
        "\n",
        "        Returns:\n",
        "            Answer based on the retrieved documents\n",
        "        \"\"\"\n",
        "        if not self.rag_chain:\n",
        "            self.initialize_rag_chain()\n",
        "\n",
        "        try:\n",
        "            return self.rag_chain.invoke(question)\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying RAG: {str(e)}\")\n",
        "            return f\"Error processing your query: {str(e)}\"\n",
        "\n",
        "# Test the class definition\n",
        "print(\"NewsRAG class defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwaa480YD8Rv",
        "outputId": "bf97ed47-ce43-4c2d-d138-64ff20258d7f"
      },
      "id": "Wwaa480YD8Rv",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NewsRAG class defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Add conversational RAG functionality\n",
        "class ConversationalNewsRAG(NewsRAG):\n",
        "    \"\"\"Extends NewsRAG with conversational capabilities.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, index_name: str, namespace: str, category: str):\n",
        "        \"\"\"Initialize using parent constructor.\"\"\"\n",
        "        super().__init__(api_key, index_name, namespace, category)\n",
        "        self.chat_history = []\n",
        "        self.conversational_rag = None\n",
        "\n",
        "    def initialize_conversational_rag(self):\n",
        "        \"\"\"Initialize a conversational RAG chain with memory.\"\"\"\n",
        "        if not self.retriever:\n",
        "            self.initialize_retriever()\n",
        "\n",
        "        self.conversational_rag = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=self.retriever,\n",
        "            return_source_documents=True\n",
        "        )\n",
        "\n",
        "        print(\"Conversational RAG chain initialized\")\n",
        "        return True\n",
        "\n",
        "    def conversational_query(self, question: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Query the conversational RAG system with chat history.\n",
        "\n",
        "        Args:\n",
        "            question: User's question\n",
        "\n",
        "        Returns:\n",
        "            Answer and source documents\n",
        "        \"\"\"\n",
        "        if not self.conversational_rag:\n",
        "            self.initialize_conversational_rag()\n",
        "\n",
        "        try:\n",
        "            # Get response using chat history\n",
        "            result = self.conversational_rag.invoke({\n",
        "                \"question\": question,\n",
        "                \"chat_history\": self.chat_history\n",
        "            })\n",
        "\n",
        "            # Update chat history\n",
        "            self.chat_history.append((question, result[\"answer\"]))\n",
        "\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Error in conversational query: {str(e)}\")\n",
        "            return {\"answer\": f\"Error processing your query: {str(e)}\", \"source_documents\": []}\n",
        "\n",
        "# Test the class definition\n",
        "print(\"ConversationalNewsRAG class defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGMFzMcZD9V7",
        "outputId": "95dc754a-f680-45ee-9d44-e3a5b9bab435"
      },
      "id": "nGMFzMcZD9V7",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConversationalNewsRAG class defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Define the state for our LangGraph agent\n",
        "class AgentState(BaseModel):\n",
        "    \"\"\"State for our RAG agent with LangGraph.\"\"\"\n",
        "    question: str = Field(description=\"The current question being asked\")\n",
        "    thoughts: str = Field(default=\"\", description=\"The agent's thoughts about how to answer the question\")\n",
        "    research_needed: bool = Field(default=False, description=\"Whether more research is needed\")\n",
        "    retrieved_documents: List[Dict] = Field(default_factory=list, description=\"Documents retrieved from the vector database\")\n",
        "    final_answer: str = Field(default=\"\", description=\"The final answer to the user's question\")\n",
        "\n",
        "print(\"AgentState class defined for LangGraph!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPqUJnHiD-df",
        "outputId": "06d31865-9fcc-4c3c-cb57-71ca0d90f1d3"
      },
      "id": "dPqUJnHiD-df",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentState class defined for LangGraph!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Create the NewsRAGAgent with LangGraph for reasoning\n",
        "class NewsRAGAgent:\n",
        "    \"\"\"A more advanced RAG agent using LangGraph for reasoning.\"\"\"\n",
        "\n",
        "    def __init__(self, news_rag: NewsRAG):\n",
        "        self.news_rag = news_rag\n",
        "        self.workflow = None\n",
        "\n",
        "    def _analyze_question(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Analyze the user's question to determine the best approach.\"\"\"\n",
        "        analysis_prompt = ChatPromptTemplate.from_template(\n",
        "            \"\"\"Analyze the following question related to {category} news:\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Think about what kind of information is needed to answer this question properly.\n",
        "            Should I retrieve specific articles or information from the database?\n",
        "\n",
        "            Thoughts:\"\"\"\n",
        "        )\n",
        "\n",
        "        thoughts = llm.invoke(\n",
        "            analysis_prompt.format(\n",
        "                question=state.question,\n",
        "                category=self.news_rag.category\n",
        "            )\n",
        "        ).content\n",
        "\n",
        "        state.thoughts = thoughts\n",
        "        state.research_needed = \"retrieve\" in thoughts.lower() or \"search\" in thoughts.lower() or \"database\" in thoughts.lower()\n",
        "        return state\n",
        "\n",
        "    def _retrieve_documents(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Retrieve relevant documents based on the question.\"\"\"\n",
        "        if state.research_needed:\n",
        "            try:\n",
        "                if not self.news_rag.retriever:\n",
        "                    self.news_rag.initialize_retriever()\n",
        "\n",
        "                docs = self.news_rag.retriever.get_relevant_documents(state.question)\n",
        "                state.retrieved_documents = [\n",
        "                    {\n",
        "                        \"content\": doc.page_content,\n",
        "                        \"metadata\": doc.metadata\n",
        "                    } for doc in docs\n",
        "                ]\n",
        "            except Exception as e:\n",
        "                print(f\"Error retrieving documents: {str(e)}\")\n",
        "        return state\n",
        "\n",
        "    def _synthesize_answer(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Synthesize an answer based on retrieved documents.\"\"\"\n",
        "        if state.retrieved_documents:\n",
        "            synthesis_prompt = ChatPromptTemplate.from_template(\n",
        "                \"\"\"You are a specialized AI assistant for {category} news analysis.\n",
        "\n",
        "                Question: {question}\n",
        "\n",
        "                Retrieved documents:\n",
        "                {documents}\n",
        "\n",
        "                Based on these documents, provide a comprehensive answer to the question.\n",
        "                Include sources where appropriate. If the retrieved documents don't contain\n",
        "                sufficient information, acknowledge this limitation.\n",
        "\n",
        "                Answer:\"\"\"\n",
        "            )\n",
        "\n",
        "            # Format documents for the prompt\n",
        "            docs_text = \"\\n\\n\".join([\n",
        "                f\"Document {i+1}:\\n{doc['content']}\\nSource: {doc['metadata'].get('source', 'Unknown')}\"\n",
        "                for i, doc in enumerate(state.retrieved_documents)\n",
        "            ])\n",
        "\n",
        "            state.final_answer = llm.invoke(\n",
        "                synthesis_prompt.format(\n",
        "                    question=state.question,\n",
        "                    documents=docs_text,\n",
        "                    category=self.news_rag.category\n",
        "                )\n",
        "            ).content\n",
        "        else:\n",
        "            state.final_answer = \"I couldn't find any relevant information to answer your question about \" + self.news_rag.category + \".\"\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _deliver_answer(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Format and deliver the final answer.\"\"\"\n",
        "        if not state.final_answer:\n",
        "            state.final_answer = \"I don't have enough information to answer your question about \" + self.news_rag.category + \".\"\n",
        "        return state\n",
        "\n",
        "    def build_workflow(self) -> StateGraph:\n",
        "        \"\"\"Build the agent workflow graph.\"\"\"\n",
        "        # Define our state graph\n",
        "        workflow = StateGraph(AgentState)\n",
        "\n",
        "        # Add nodes\n",
        "        workflow.add_node(\"analyze_question\", self._analyze_question)\n",
        "        workflow.add_node(\"retrieve_documents\", self._retrieve_documents)\n",
        "        workflow.add_node(\"synthesize_answer\", self._synthesize_answer)\n",
        "        workflow.add_node(\"deliver_answer\", self._deliver_answer)\n",
        "\n",
        "        # Define edges\n",
        "        workflow.add_edge(\"analyze_question\", \"retrieve_documents\")\n",
        "        workflow.add_conditional_edges(\n",
        "            \"retrieve_documents\",\n",
        "            lambda state: \"synthesize_answer\" if state.retrieved_documents else \"deliver_answer\",\n",
        "            {\n",
        "                True: \"synthesize_answer\",\n",
        "                False: \"deliver_answer\"\n",
        "            }\n",
        "        )\n",
        "        workflow.add_edge(\"synthesize_answer\", \"deliver_answer\")\n",
        "        workflow.add_edge(\"deliver_answer\", END)\n",
        "\n",
        "        # Set entry point\n",
        "        workflow.set_entry_point(\"analyze_question\")\n",
        "\n",
        "        self.workflow = workflow.compile()\n",
        "        return self.workflow\n",
        "\n",
        "    def run(self, question: str) -> str:\n",
        "        \"\"\"Run the agent with a question.\"\"\"\n",
        "        if not self.workflow:\n",
        "            self.build_workflow()\n",
        "\n",
        "        # Create initial state with the question\n",
        "        initial_state = AgentState(question=question)\n",
        "\n",
        "        # Run the workflow\n",
        "        final_state = self.workflow.invoke(initial_state)\n",
        "\n",
        "        return final_state.final_answer\n",
        "\n",
        "# Test the class definition\n",
        "print(\"NewsRAGAgent class with LangGraph defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOfNV6o5D_lw",
        "outputId": "b3ad41bd-2aab-4344-ae1f-f9109b8daf8c"
      },
      "id": "aOfNV6o5D_lw",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NewsRAGAgent class with LangGraph defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Initialize and test the basic RAG system\n",
        "# Initialize the RAG system\n",
        "rag = NewsRAG(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    index_name=PINECONE_INDEX_NAME,\n",
        "    namespace=NAMESPACE,\n",
        "    category=CATEGORY\n",
        ")\n",
        "\n",
        "# Connect to Pinecone\n",
        "print(\"Connecting to Pinecone...\")\n",
        "if rag.connect_to_pinecone():\n",
        "    print(\"Successfully connected to Pinecone!\")\n",
        "\n",
        "    # Initialize the retriever\n",
        "    print(\"Initializing retriever...\")\n",
        "    if rag.initialize_retriever():\n",
        "        print(\"Retriever initialized!\")\n",
        "\n",
        "        # Initialize the RAG chain\n",
        "        print(\"Initializing RAG chain...\")\n",
        "        if rag.initialize_rag_chain():\n",
        "            print(\"RAG chain initialized!\")\n",
        "\n",
        "            # Test with a basic query\n",
        "            test_question = \"What are the recent trends in finance?\"\n",
        "            print(f\"\\nTesting with question: '{test_question}'\")\n",
        "            answer = rag.query(test_question)\n",
        "            print(f\"\\nAnswer: {answer}\")\n",
        "        else:\n",
        "            print(\"Failed to initialize RAG chain.\")\n",
        "    else:\n",
        "        print(\"Failed to initialize retriever.\")\n",
        "else:\n",
        "    print(\"Failed to connect to Pinecone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3jjxMDbEAug",
        "outputId": "20daef4a-4e1f-4e34-a26d-dc0b02b25a19"
      },
      "id": "P3jjxMDbEAug",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Pinecone...\n",
            "Connected to Pinecone index: cus635\n",
            "Successfully connected to Pinecone!\n",
            "Initializing retriever...\n",
            "Error initializing retriever: client should be an instance of pinecone.Index, got <class 'pinecone.data.index.Index'>\n",
            "Failed to initialize retriever.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Test the conversational RAG system\n",
        "# Initialize the conversational RAG\n",
        "conv_rag = ConversationalNewsRAG(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    index_name=PINECONE_INDEX_NAME,\n",
        "    namespace=NAMESPACE,\n",
        "    category=CATEGORY\n",
        ")\n",
        "\n",
        "# Connect to Pinecone and initialize\n",
        "print(\"Connecting to Pinecone for conversational RAG...\")\n",
        "if conv_rag.connect_to_pinecone() and conv_rag.initialize_retriever():\n",
        "    print(\"Conversational RAG system ready!\")\n",
        "\n",
        "    # Test with a sequence of questions\n",
        "    questions = [\n",
        "        \"What are the major financial markets performing right now?\",\n",
        "        \"What factors are affecting their performance?\",\n",
        "        \"Which companies are mentioned most frequently in recent financial news?\"\n",
        "    ]\n",
        "\n",
        "    for i, question in enumerate(questions):\n",
        "        print(f\"\\nQuestion {i+1}: {question}\")\n",
        "        result = conv_rag.conversational_query(question)\n",
        "        print(f\"\\nAnswer: {result['answer']}\")\n",
        "        print(\"\\nSources:\")\n",
        "        for j, doc in enumerate(result.get(\"source_documents\", [])[:2]):\n",
        "            print(f\"  {j+1}. {doc.metadata.get('title', 'Unknown title')} - {doc.metadata.get('source', 'Unknown source')}\")\n",
        "else:\n",
        "    print(\"Failed to initialize the conversational RAG system.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnfopd9lECHs",
        "outputId": "4b5b3279-789a-4c8d-fc8c-f1dcf18c6663"
      },
      "id": "qnfopd9lECHs",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Pinecone for conversational RAG...\n",
            "Connected to Pinecone index: cus635\n",
            "Error initializing retriever: client should be an instance of pinecone.Index, got <class 'pinecone.data.index.Index'>\n",
            "Failed to initialize the conversational RAG system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Test the LangGraph agent\n",
        "# Initialize the RAG base for our agent\n",
        "rag_base = NewsRAG(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    index_name=PINECONE_INDEX_NAME,\n",
        "    namespace=NAMESPACE,\n",
        "    category=CATEGORY\n",
        ")\n",
        "\n",
        "# Connect and initialize\n",
        "print(\"Setting up the base RAG system for our agent...\")\n",
        "if rag_base.connect_to_pinecone() and rag_base.initialize_retriever():\n",
        "    print(\"Base RAG system ready for the agent!\")\n",
        "\n",
        "    # Create the LangGraph agent\n",
        "    print(\"Creating LangGraph agent...\")\n",
        "    agent = NewsRAGAgent(rag_base)\n",
        "\n",
        "    # Run the agent with a question\n",
        "    test_questions = [\n",
        "        \"What is the current state of cryptocurrency markets?\",\n",
        "        \"How are regulatory changes affecting the financial sector?\",\n",
        "        \"What are the implications of recent financial news for retail investors?\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nTesting the LangGraph agent with different questions:\")\n",
        "    for i, question in enumerate(test_questions):\n",
        "        print(f\"\\nQuestion {i+1}: {question}\")\n",
        "        answer = agent.run(question)\n",
        "        print(f\"\\nAgent response:\\n{answer}\")\n",
        "else:\n",
        "    print(\"Failed to set up the base RAG system for the agent.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG0I05YuEDfP",
        "outputId": "45a6b07f-fdb5-4db1-d219-fc68f9351086"
      },
      "id": "JG0I05YuEDfP",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up the base RAG system for our agent...\n",
            "Connected to Pinecone index: cus635\n",
            "Error initializing retriever: client should be an instance of pinecone.Index, got <class 'pinecone.data.index.Index'>\n",
            "Failed to set up the base RAG system for the agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Create a user-friendly interface for interacting with our RAG systems\n",
        "def display_welcome():\n",
        "    \"\"\"Display a welcome message with instructions.\"\"\"\n",
        "    welcome_text = \"\"\"\n",
        "    # 📰 News RAG Agent - Interactive Demo 📰\n",
        "\n",
        "    This system uses RAG (Retrieval-Augmented Generation) to answer questions about news articles.\n",
        "\n",
        "    Choose a mode:\n",
        "    1. Basic RAG - Simple question answering\n",
        "    2. Conversational RAG - Maintains context across questions\n",
        "    3. LangGraph Agent - Advanced reasoning with multiple steps\n",
        "\n",
        "    Type 'exit' at any time to quit.\n",
        "    \"\"\"\n",
        "    print(welcome_text)\n",
        "\n",
        "def run_interactive_session():\n",
        "    \"\"\"Run an interactive session with the user.\"\"\"\n",
        "    display_welcome()\n",
        "\n",
        "    # Initialize all systems\n",
        "    print(\"\\nInitializing all RAG systems...\")\n",
        "\n",
        "    # Basic RAG\n",
        "    basic_rag = NewsRAG(\n",
        "        api_key=PINECONE_API_KEY,\n",
        "        index_name=PINECONE_INDEX_NAME,\n",
        "        namespace=NAMESPACE,\n",
        "        category=CATEGORY\n",
        "    )\n",
        "\n",
        "    # Conversational RAG\n",
        "    conv_rag = ConversationalNewsRAG(\n",
        "        api_key=PINECONE_API_KEY,\n",
        "        index_name=PINECONE_INDEX_NAME,\n",
        "        namespace=NAMESPACE,\n",
        "        category=CATEGORY\n",
        "    )\n",
        "\n",
        "    # LangGraph Agent\n",
        "    agent_base = NewsRAG(\n",
        "        api_key=PINECONE_API_KEY,\n",
        "        index_name=PINECONE_INDEX_NAME,\n",
        "        namespace=NAMESPACE,\n",
        "        category=CATEGORY\n",
        "    )\n",
        "    agent = NewsRAGAgent(agent_base)\n",
        "\n",
        "    # Connect everything to Pinecone\n",
        "    if (basic_rag.connect_to_pinecone() and\n",
        "        basic_rag.initialize_retriever() and\n",
        "        basic_rag.initialize_rag_chain() and\n",
        "        conv_rag.connect_to_pinecone() and\n",
        "        conv_rag.initialize_retriever() and\n",
        "        conv_rag.initialize_conversational_rag() and\n",
        "        agent_base.connect_to_pinecone() and\n",
        "        agent_base.initialize_retriever() and\n",
        "        agent.build_workflow()):\n",
        "\n",
        "        print(\"\\nAll systems ready!\")\n",
        "\n",
        "        mode = input(\"\\nChoose a mode (1, 2, or 3): \")\n",
        "\n",
        "        while True:\n",
        "            if mode not in [\"1\", \"2\", \"3\"]:\n",
        "                mode = input(\"Please enter a valid mode (1, 2, or 3): \")\n",
        "                continue\n",
        "\n",
        "            # Display the selected mode\n",
        "            mode_names = {\n",
        "                \"1\": \"Basic RAG\",\n",
        "                \"2\": \"Conversational RAG\",\n",
        "                \"3\": \"LangGraph Agent\"\n",
        "            }\n",
        "            print(f\"\\n[{mode_names[mode]} Mode]\")\n",
        "\n",
        "            # Get the question\n",
        "            question = input(\"\\nYour question: \")\n",
        "\n",
        "            if question.lower() == 'exit':\n",
        "                break\n",
        "\n",
        "            if question.lower() == 'change mode':\n",
        "                mode = input(\"\\nChoose a new mode (1, 2, or 3): \")\n",
        "                continue\n",
        "\n",
        "            print(\"\\nProcessing...\")\n",
        "\n",
        "            # Process based on the selected mode\n",
        "            if mode == \"1\":\n",
        "                answer = basic_rag.query(question)\n",
        "                print(f\"\\nAnswer: {answer}\")\n",
        "\n",
        "            elif mode == \"2\":\n",
        "                result = conv_rag.conversational_query(question)\n",
        "                print(f\"\\nAnswer: {result['answer']}\")\n",
        "                print(\"\\nSources:\")\n",
        "                for i, doc in enumerate(result.get(\"source_documents\", [])[:2]):\n",
        "                    print(f\"  {i+1}. {doc.metadata.get('title', 'Unknown title')} - {doc.metadata.get('source', 'Unknown source')}\")\n",
        "\n",
        "            elif mode == \"3\":\n",
        "                answer = agent.run(question)\n",
        "                print(f\"\\nAgent response:\\n{answer}\")\n",
        "\n",
        "            print(\"\\nType 'change mode' to switch modes or 'exit' to quit.\")\n",
        "    else:\n",
        "        print(\"Failed to initialize RAG systems.\")\n",
        "\n",
        "# Run the interactive session\n",
        "run_interactive_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6OczKp4EHHe",
        "outputId": "ada190e0-5bba-4d61-c03c-d9d5d740112c"
      },
      "id": "Y6OczKp4EHHe",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    # 📰 News RAG Agent - Interactive Demo 📰\n",
            "    \n",
            "    This system uses RAG (Retrieval-Augmented Generation) to answer questions about news articles.\n",
            "    \n",
            "    Choose a mode:\n",
            "    1. Basic RAG - Simple question answering\n",
            "    2. Conversational RAG - Maintains context across questions\n",
            "    3. LangGraph Agent - Advanced reasoning with multiple steps\n",
            "    \n",
            "    Type 'exit' at any time to quit.\n",
            "    \n",
            "\n",
            "Initializing all RAG systems...\n",
            "Connected to Pinecone index: cus635\n",
            "Error initializing retriever: client should be an instance of pinecone.Index, got <class 'pinecone.data.index.Index'>\n",
            "Failed to initialize RAG systems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "60VUroWhEJ3K"
      },
      "id": "60VUroWhEJ3K",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}