{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben45123/AWS-Lambda-Research/blob/main/rag_news_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install the updated packages\n",
        "!pip install pinecone-client==6.0.2 langchain langchain_openai openai langgraph pydantic langchain_pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRW2E-XKD2sM",
        "outputId": "b259fcfe-50ea-446d-ae61-69456576cf82"
      },
      "id": "fRW2E-XKD2sM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following yanked versions: 2.2.0, 2.2.5\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pinecone-client==6.0.2 (from versions: 0.7.5, 0.7.11, 0.7.13, 0.7.16, 0.7.17, 0.7.26, 0.7.27, 0.7.42, 0.7.44, 0.7.46, 0.8.1, 0.8.5, 0.8.7, 0.8.8, 0.8.9, 0.8.10, 0.8.27, 0.8.33, 0.8.34, 0.8.36, 0.8.37, 0.8.38, 0.8.39b0, 0.8.39rc0, 0.8.39, 0.8.53, 0.8.56, 0.8.58, 0.8.59, 0.8.60, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.0.10, 2.0.11, 2.0.12, 2.0.13, 2.1.0, 2.2.1, 2.2.2rc1, 2.2.2rc3, 2.2.2rc4, 2.2.2, 2.2.3rc1, 2.2.3, 2.2.4, 2.2.5rc2, 2.3.0.dev1, 3.0.0.dev1, 3.0.0.dev2, 3.0.0.dev3, 3.0.0.dev4, 3.0.0.dev5, 3.0.0.dev6, 3.0.0.dev7, 3.0.0.dev8, 3.0.0.dev9, 3.0.0.dev10, 3.0.0rc2, 3.0.0rc3, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0.dev1, 3.1.0, 3.2.0, 3.2.1, 3.2.2, 4.0.0, 4.1.0, 4.1.1, 4.1.2, 5.0.0, 5.0.1, 6.0.0.dev3, 6.0.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pinecone-client==6.0.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import libraries and set API keys\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "import pinecone\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set API keys from Colab userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')  # Or use your provided key if saved\n",
        "# If you're using the key from your provided code\n",
        "# PINECONE_API_KEY = \"pcsk_6akU8Z_2BXXXDSBKbvFCn4sciNM2FeJC6PwAt6wFwQeQjoJKDSjysRbtyBAdUfRv6z87e6\"\n",
        "\n",
        "# Set environment variables (some LangChain components use these)\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "# Configuration for Pinecone\n",
        "PINECONE_INDEX_NAME = \"cus635\"\n",
        "NAMESPACE = \"Team_1\"\n",
        "CATEGORY = \"Finance\"\n",
        "\n",
        "print(\"API keys loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLmzYwKpD3Jj",
        "outputId": "46e1b697-3b37-4056-eb32-93c8a6370064"
      },
      "id": "oLmzYwKpD3Jj",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Import LangChain and LangGraph components\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "# Use the correct import for Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Print version for debugging\n",
        "import pinecone\n",
        "print(f\"Pinecone SDK version: {pinecone.__version__}\")\n",
        "\n",
        "# Initialize models\n",
        "embeddings = OpenAIEmbeddings()\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "print(\"LangChain components initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpG4H3PkD7I4",
        "outputId": "6958e17b-e60b-4bd6-dc13-2049010a17dc"
      },
      "id": "SpG4H3PkD7I4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone SDK version: 6.0.2\n",
            "LangChain components initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Create the NewsRAG class for basic RAG functionality\n",
        "class NewsRAG:\n",
        "    \"\"\"\n",
        "    A RAG system that uses LangChain to answer questions based on news articles\n",
        "    stored in Pinecone.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, index_name: str, namespace: str, category: str):\n",
        "        \"\"\"\n",
        "        Initialize the NewsRAG with Pinecone credentials and team information.\n",
        "\n",
        "        Args:\n",
        "            api_key: Pinecone API key\n",
        "            index_name: Pinecone index name\n",
        "            namespace: Namespace (team name)\n",
        "            category: News category\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.index_name = index_name\n",
        "        self.namespace = namespace\n",
        "        self.category = category\n",
        "        self.pinecone_index = None\n",
        "        self.retriever = None\n",
        "        self.rag_chain = None\n",
        "        self.conversational_memory = []\n",
        "\n",
        "    def connect_to_pinecone(self):\n",
        "        \"\"\"Connect to Pinecone and initialize the index.\"\"\"\n",
        "        try:\n",
        "            # Initialize Pinecone\n",
        "            pinecone_client = pinecone.Pinecone(api_key=self.api_key)\n",
        "            self.pinecone_index = pinecone_client.Index(self.index_name)\n",
        "            print(f\"Connected to Pinecone index: {self.index_name}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error connecting to Pinecone: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def initialize_retriever(self):\n",
        "        \"\"\"Initialize the retriever from the Pinecone index.\"\"\"\n",
        "        try:\n",
        "            # Make sure we have the Pinecone API key in env vars\n",
        "            os.environ[\"PINECONE_API_KEY\"] = self.api_key\n",
        "\n",
        "            # Create a vector store using the updated PineconeVectorStore class\n",
        "            vectorstore = PineconeVectorStore(\n",
        "                embedding=embeddings,\n",
        "                index_name=self.index_name,\n",
        "                namespace=self.namespace,\n",
        "                text_key=\"text\"\n",
        "            )\n",
        "\n",
        "            # Create the retriever with filters\n",
        "            self.retriever = vectorstore.as_retriever(\n",
        "                search_kwargs={\n",
        "                    \"k\": 5,\n",
        "                    \"filter\": {\"category\": self.category}\n",
        "                }\n",
        "            )\n",
        "\n",
        "            print(f\"Retriever initialized for category: {self.category}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing retriever: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def initialize_rag_chain(self):\n",
        "        \"\"\"Initialize the basic RAG chain.\"\"\"\n",
        "        # Define the prompt template\n",
        "        template = \"\"\"You are an AI assistant specialized in analyzing news articles in the {category} domain.\n",
        "        Use the following retrieved news articles to answer the question.\n",
        "\n",
        "        Retrieved articles:\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer the question based on the retrieved articles. If the retrieved articles don't contain the information\n",
        "        needed to answer the question accurately, acknowledge the limitations and provide the best answer possible\n",
        "        based on available information. Include relevant sources in your response.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create the prompt\n",
        "        prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "        # Create the RAG chain\n",
        "        self.rag_chain = (\n",
        "            {\"context\": self.retriever, \"question\": RunnablePassthrough(), \"category\": lambda _: self.category}\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        print(\"Basic RAG chain initialized\")\n",
        "        return True\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Query the RAG system with a question.\n",
        "\n",
        "        Args:\n",
        "            question: User's question\n",
        "\n",
        "        Returns:\n",
        "            Answer based on the retrieved documents\n",
        "        \"\"\"\n",
        "        if not self.rag_chain:\n",
        "            self.initialize_rag_chain()\n",
        "\n",
        "        try:\n",
        "            return self.rag_chain.invoke(question)\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying RAG: {str(e)}\")\n",
        "            return f\"Error processing your query: {str(e)}\""
      ],
      "metadata": {
        "id": "Wwaa480YD8Rv"
      },
      "id": "Wwaa480YD8Rv",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Add conversational RAG functionality\n",
        "class ConversationalNewsRAG(NewsRAG):\n",
        "    \"\"\"Extends NewsRAG with conversational capabilities.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, index_name: str, namespace: str, category: str):\n",
        "        \"\"\"Initialize using parent constructor.\"\"\"\n",
        "        super().__init__(api_key, index_name, namespace, category)\n",
        "        self.chat_history = []\n",
        "        self.conversational_rag = None\n",
        "\n",
        "    def initialize_conversational_rag(self):\n",
        "        \"\"\"Initialize a conversational RAG chain with memory.\"\"\"\n",
        "        if not self.retriever:\n",
        "            self.initialize_retriever()\n",
        "\n",
        "        self.conversational_rag = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=self.retriever,\n",
        "            return_source_documents=True\n",
        "        )\n",
        "\n",
        "        print(\"Conversational RAG chain initialized\")\n",
        "        return True\n",
        "\n",
        "    def conversational_query(self, question: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Query the conversational RAG system with chat history.\n",
        "\n",
        "        Args:\n",
        "            question: User's question\n",
        "\n",
        "        Returns:\n",
        "            Answer and source documents\n",
        "        \"\"\"\n",
        "        if not self.conversational_rag:\n",
        "            self.initialize_conversational_rag()\n",
        "\n",
        "        try:\n",
        "            # Get response using chat history\n",
        "            result = self.conversational_rag.invoke({\n",
        "                \"question\": question,\n",
        "                \"chat_history\": self.chat_history\n",
        "            })\n",
        "\n",
        "            # Update chat history\n",
        "            self.chat_history.append((question, result[\"answer\"]))\n",
        "\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Error in conversational query: {str(e)}\")\n",
        "            return {\"answer\": f\"Error processing your query: {str(e)}\", \"source_documents\": []}"
      ],
      "metadata": {
        "id": "nGMFzMcZD9V7"
      },
      "id": "nGMFzMcZD9V7",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Debugging Cell - Run this first to validate connections\n",
        "import os\n",
        "import pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Print version information\n",
        "print(f\"Pinecone SDK version: {pinecone.__version__}\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "try:\n",
        "    pinecone_client = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
        "    print(\"Pinecone client initialized successfully\")\n",
        "\n",
        "    # List available indexes\n",
        "    indexes = pinecone_client.list_indexes()\n",
        "    print(f\"Available indexes: {indexes}\")\n",
        "\n",
        "    # Connect to the specific index\n",
        "    index = pinecone_client.Index(PINECONE_INDEX_NAME)\n",
        "    print(f\"Connected to index: {PINECONE_INDEX_NAME}\")\n",
        "\n",
        "    # Verify index stats\n",
        "    stats = index.describe_index_stats()\n",
        "    print(f\"Index stats: {stats}\")\n",
        "\n",
        "    # Try initializing the vector store\n",
        "    try:\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        vectorstore = PineconeVectorStore(\n",
        "            embedding=embeddings,\n",
        "            index_name=PINECONE_INDEX_NAME,\n",
        "            namespace=NAMESPACE,\n",
        "            text_key=\"text\"\n",
        "        )\n",
        "        print(\"Vector store initialized successfully!\")\n",
        "\n",
        "        # Try creating a retriever\n",
        "        retriever = vectorstore.as_retriever(\n",
        "            search_kwargs={\n",
        "                \"k\": 5,\n",
        "                \"filter\": {\"category\": CATEGORY}\n",
        "            }\n",
        "        )\n",
        "        print(\"Retriever created successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vector store: {str(e)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to Pinecone: {str(e)}\")\n",
        "\n",
        "# Test with a basic connection\n",
        "print(\"\\nTesting with a simple query...\")\n",
        "try:\n",
        "    # Create a test embedding\n",
        "    test_embedding = embeddings.embed_query(\"Test query about finance\")\n",
        "    # Try a simple query to the index\n",
        "    query_response = index.query(\n",
        "        vector=test_embedding,\n",
        "        top_k=1,\n",
        "        namespace=NAMESPACE,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    print(\"Query successful!\")\n",
        "    print(f\"First result metadata: {query_response.matches[0].metadata if query_response.matches else 'No matches'}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error querying index: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPqUJnHiD-df",
        "outputId": "50947756-3a9d-496e-88c9-6e06a82fcbe7"
      },
      "id": "dPqUJnHiD-df",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone SDK version: 6.0.2\n",
            "Pinecone client initialized successfully\n",
            "Available indexes: [{\n",
            "    \"name\": \"cus635\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"cus635-g311jqa.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"cloud\": \"aws\",\n",
            "            \"region\": \"us-east-1\"\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1024,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null,\n",
            "    \"embed\": {\n",
            "        \"model\": \"llama-text-embed-v2\",\n",
            "        \"field_map\": {\n",
            "            \"text\": \"text\"\n",
            "        },\n",
            "        \"dimension\": 1024,\n",
            "        \"metric\": \"cosine\",\n",
            "        \"write_parameters\": {\n",
            "            \"dimension\": 1024.0,\n",
            "            \"input_type\": \"passage\",\n",
            "            \"truncate\": \"END\"\n",
            "        },\n",
            "        \"read_parameters\": {\n",
            "            \"dimension\": 1024.0,\n",
            "            \"input_type\": \"query\",\n",
            "            \"truncate\": \"END\"\n",
            "        },\n",
            "        \"vector_type\": \"dense\"\n",
            "    }\n",
            "}, {\n",
            "    \"name\": \"ciro\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"ciro-g311jqa.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"cloud\": \"aws\",\n",
            "            \"region\": \"us-east-1\"\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": {\n",
            "        \"embedding_model\": \"text-embedding-3-small\"\n",
            "    }\n",
            "}, {\n",
            "    \"name\": \"ciro-openai\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"ciro-openai-g311jqa.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"cloud\": \"aws\",\n",
            "            \"region\": \"us-east-1\"\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}]\n",
            "Connected to index: cus635\n",
            "Index stats: {'dimension': 1024,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'TEAM_3': {'vector_count': 436},\n",
            "                'Team 2': {'vector_count': 103},\n",
            "                'Team_1': {'vector_count': 137},\n",
            "                'team 6': {'vector_count': 10}},\n",
            " 'total_vector_count': 686,\n",
            " 'vector_type': 'dense'}\n",
            "Vector store initialized successfully!\n",
            "Retriever created successfully!\n",
            "\n",
            "Testing with a simple query...\n",
            "Error querying index: (400)\n",
            "Reason: Bad Request\n",
            "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 30 Apr 2025 19:10:33 GMT', 'Content-Type': 'application/json', 'Content-Length': '104', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '123', 'x-pinecone-request-id': '9138161606964437408', 'x-envoy-upstream-service-time': '1', 'server': 'envoy'})\n",
            "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 1536 does not match the dimension of the index 1024\",\"details\":[]}\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}